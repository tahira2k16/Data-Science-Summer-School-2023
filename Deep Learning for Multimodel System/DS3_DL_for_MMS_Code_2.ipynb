{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Predicting House Prizes\n",
        "* Chris Arnold (Cardiff University)\n",
        "* DS3 Data Science Summer School\n",
        "* August 25th, 2023"
      ],
      "metadata": {
        "id": "wb5I7ouAWnrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Housekeeping\n",
        "from IPython.display import Image\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import locale\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import concatenate\n",
        "\n"
      ],
      "metadata": {
        "id": "ksteo2mqhPDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paper Source\n",
        "This is [the link](https://arxiv.org/pdf/1609.08399.pdf) to Eman H. Ahmed, Mohamed N. Moustafa (2016) \"House price estimation from visual and textual features\"\n",
        "\n",
        "If you want to download the data yourself, clone into the original data set use:\n"
      ],
      "metadata": {
        "id": "7Pc70TW2fP7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "metadata": {
        "id": "rpALoj9WT1df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They predict house prizes on the basis of\n",
        "* images from the bedroom, bathroom, kitchen and the front of the house\n",
        "* And information about the number of bedrooms, bathrooms, the area, the zipcode (FE)"
      ],
      "metadata": {
        "id": "x06iP_p8fj2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You might have to mount your drive and then cd into it\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "xNlTMRNuuk9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where are we?\n",
        "%pwd"
      ],
      "metadata": {
        "id": "q4A22MHGu8S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "wd31rx4Cf9w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Houses-dataset/Houses\\ Dataset"
      ],
      "metadata": {
        "id": "YY4Yf6MXOddN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "uG0_8R5BOqpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at a house\n",
        "Image(filename='220_frontal.jpg')"
      ],
      "metadata": {
        "id": "SO2RuVZef9oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(filename='220_kitchen.jpg')"
      ],
      "metadata": {
        "id": "TiEt9pcV-zdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(filename='220_bedroom.jpg')"
      ],
      "metadata": {
        "id": "8WWsv0DNhvO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(filename='220_bathroom.jpg')"
      ],
      "metadata": {
        "id": "O8PjqXw0hwVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Attributes"
      ],
      "metadata": {
        "id": "-c5uhkSL-uM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_house_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then\n",
        "\t# load it using Pandas\n",
        "\tcols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "\tdf = pd.read_csv(inputPath, sep=\" \", header=None, names=cols)\n",
        "\t# determine (1) the unique zip codes and (2) the number of data\n",
        "\t# points with each zip code\n",
        "\tzipcodes = df[\"zipcode\"].value_counts().keys().tolist()\n",
        "\tcounts = df[\"zipcode\"].value_counts().tolist()\n",
        "\t# loop over each of the unique zip codes and their corresponding\n",
        "\t# count\n",
        "\tfor (zipcode, count) in zip(zipcodes, counts):\n",
        "\t\t# the zip code counts for our housing dataset is *extremely*\n",
        "\t\t# unbalanced (some only having 1 or 2 houses per zip code)\n",
        "\t\t# so let's sanitize our data by removing any houses with less\n",
        "\t\t# than 25 houses per zip code\n",
        "\t\tif count < 25:\n",
        "\t\t\tidxs = df[df[\"zipcode\"] == zipcode].index\n",
        "\t\t\tdf.drop(idxs, inplace=True)\n",
        "\t# return the data frame\n",
        "\treturn df"
      ],
      "metadata": {
        "id": "FZ2eElaV2gQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process that data and get reshaped data\n",
        "def process_house_attributes(df, train, test):\n",
        "\t# initialize the column names of the continuous data\n",
        "\tcontinuous = [\"bedrooms\", \"bathrooms\", \"area\"]\n",
        "\t# performin min-max scaling each continuous feature column to\n",
        "\t# the range [0, 1]\n",
        "\tcs = MinMaxScaler()\n",
        "\ttrainContinuous = cs.fit_transform(train[continuous])\n",
        "\ttestContinuous = cs.transform(test[continuous])\n",
        "\t# one-hot encode the zip code categorical data (by definition of\n",
        "\t# one-hot encoding, all output features are now in the range [0, 1])\n",
        "\tzipBinarizer = LabelBinarizer().fit(df[\"zipcode\"])\n",
        "\ttrainCategorical = zipBinarizer.transform(train[\"zipcode\"])\n",
        "\ttestCategorical = zipBinarizer.transform(test[\"zipcode\"])\n",
        "\t# construct our training and testing data points by concatenating\n",
        "\t# the categorical features with the continuous features\n",
        "\ttrainX = np.hstack([trainCategorical, trainContinuous])\n",
        "\ttestX = np.hstack([testCategorical, testContinuous])\n",
        "\t# return the concatenated training and testing data\n",
        "\treturn (trainX, testX)"
      ],
      "metadata": {
        "id": "5xJN4kg5_u4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Images\n"
      ],
      "metadata": {
        "id": "EeM6VifD-1LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This loads the data and concatenates the four images into one\n",
        "def load_house_images(df, inputPath):\n",
        "\t# initialize our images array (i.e., the house images themselves)\n",
        "\timages = []\n",
        "\t# loop over the indexes of the houses\n",
        "\tfor i in df.index.values:\n",
        "\t\t# find the four images for the house and sort the file paths,\n",
        "\t\t# ensuring the four are always in the *same order*\n",
        "\t\tbasePath = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "\t\thousePaths = sorted(list(glob.glob(basePath)))\n",
        "\t\t# initialize our list of input images along with the output image\n",
        "\t\t# after *combining* the four input images\n",
        "\t\tinputImages = []\n",
        "\t\toutputImage = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "\t\t# loop over the input house paths\n",
        "\t\tfor housePath in housePaths:\n",
        "\t\t\t# load the input image, resize it to be 32 32, and then\n",
        "\t\t\t# update the list of input images\n",
        "\t\t\timage = cv2.imread(housePath)\n",
        "\t\t\timage = cv2.resize(image, (32, 32))\n",
        "\t\t\tinputImages.append(image)\n",
        "\t\t# tile the four input images in the output image such the first\n",
        "\t\t# image goes in the top-right corner, the second image in the\n",
        "\t\t# top-left corner, the third image in the bottom-right corner,\n",
        "\t\t# and the final image in the bottom-left corner\n",
        "\t\toutputImage[0:32, 0:32] = inputImages[0]\n",
        "\t\toutputImage[0:32, 32:64] = inputImages[1]\n",
        "\t\toutputImage[32:64, 32:64] = inputImages[2]\n",
        "\t\toutputImage[32:64, 0:32] = inputImages[3]\n",
        "\t\t# add the tiled image to our set of images the network will be\n",
        "\t\t# trained on\n",
        "\t\timages.append(outputImage)\n",
        "\t# return our set of images\n",
        "\treturn np.array(images)"
      ],
      "metadata": {
        "id": "xrArTFJI2Yl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Model"
      ],
      "metadata": {
        "id": "TpRpXz4LJ2kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "def create_mlp(dim, regress=False):\n",
        "\t# define our MLP network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "\tmodel.add(Dense(4, activation=\"relu\"))\n",
        "\t# check to see if the regression node should be added\n",
        "\tif regress:\n",
        "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
        "\t# return our model\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "igM9WDZLJ1gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
        "  # initialize the input shape and channel dimension, assuming\n",
        "  # TensorFlow/channels-last ordering\n",
        "  inputShape = (height, width, depth)\n",
        "  chanDim = -1\n",
        "  # define the model input\n",
        "  inputs = Input(shape=inputShape)\n",
        "\n",
        "  # CONV => RELU => BN => POOL\n",
        "  x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(axis=chanDim)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(axis=chanDim)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(axis=chanDim)(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # flatten the volume, then FC => RELU => BN => DROPOUT\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(16)(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  x = BatchNormalization(axis=chanDim)(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  # apply another FC layer, this one to match the number of nodes\n",
        "  # coming out of the MLP\n",
        "  x = Dense(4)(x)\n",
        "  x = Activation(\"relu\")(x)\n",
        "  # check to see if the regression node should be added\n",
        "  if regress:\n",
        "  \tx = Dense(1, activation=\"linear\")(x)\n",
        "  # construct the CNN\n",
        "  model = Model(inputs, x)\n",
        "  # return the CNN\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "-uuVyDUnJ1qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construct the Data Pipeline\n"
      ],
      "metadata": {
        "id": "bFGOtq6gCSRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the attributes\n",
        "inputPath_house_data = \"HousesInfo.txt\"\n",
        "df_house_att = load_house_attributes(inputPath_house_data)"
      ],
      "metadata": {
        "id": "hMh8PfJVSaKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the images\n",
        "df_house_img = load_house_images(df_house_att, \"/content/drive/MyDrive/Colab Notebooks/Houses-dataset/House Dataset\")"
      ],
      "metadata": {
        "id": "BUumSv5LC0jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# norm the data to be between 0 and 1\n",
        "df_house_img = df_house_img / 255.0"
      ],
      "metadata": {
        "id": "cyudMERMS4TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the Data"
      ],
      "metadata": {
        "id": "bCKufiDOjr8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "split = train_test_split(df_house_att, df_house_img, test_size=0.25, random_state=42)\n",
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n"
      ],
      "metadata": {
        "id": "7NPWlDnRCHjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: Size of the data here!\n",
        "print(trainAttrX.shape)\n",
        "print(testAttrX.shape)"
      ],
      "metadata": {
        "id": "N-aS3Ye_j4v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the largest house price in the training set and use it to\n",
        "# scale our house prices to the range [0, 1] (will lead to better\n",
        "# training and convergence)\n",
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice\n"
      ],
      "metadata": {
        "id": "-NMbdS-HCHgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process the house attributes data by performing min-max scaling\n",
        "# on continuous features, one-hot encoding on categorical features,\n",
        "# and then finally concatenating them together\n",
        "(trainAttrX, testAttrX) = process_house_attributes(df_house_att,\n",
        "\ttrainAttrX, testAttrX)\n"
      ],
      "metadata": {
        "id": "1TjlRT7kCHdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Model"
      ],
      "metadata": {
        "id": "71Z6vwBYU4yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the MLP and CNN models\n",
        "mlp = create_mlp(trainAttrX.shape[1], regress=False)\n",
        "cnn = create_cnn(64, 64, 3, regress=False)\n"
      ],
      "metadata": {
        "id": "trti0vlwU4Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.summary()"
      ],
      "metadata": {
        "id": "SHPkpNIkWfzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.summary()"
      ],
      "metadata": {
        "id": "YNhwmlO-Wfmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the input to our final set of layers as the *output* of both\n",
        "# the MLP and CNN\n",
        "combinedInput = concatenate([mlp.output, cnn.output])\n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being the regression head\n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "x = Dense(1, activation=\"linear\")(x)\n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
      ],
      "metadata": {
        "id": "oAHjOrsjCHa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "48SsDGnICHUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "sK4EiolOYieP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model using mean absolute percentage error as our loss,\n",
        "# implying that we seek to minimize the absolute percentage difference\n",
        "# between our price *predictions* and the *actual prices*\n",
        "opt = Adam(learning_rate=1e-3)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n"
      ],
      "metadata": {
        "id": "LgRbbsXzCHRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "# for a good fit, run 200 epochs\n",
        "model.fit(\n",
        "\tx=[trainAttrX, trainImagesX], y=trainY,\n",
        "\tvalidation_data=([testAttrX, testImagesX], testY),\n",
        "\tepochs=200, batch_size=8)\n"
      ],
      "metadata": {
        "id": "72FSGytjCHOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on the testing data\n",
        "preds = model.predict([testAttrX, testImagesX])"
      ],
      "metadata": {
        "id": "xmoiB2xZCHLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the difference between the *predicted* house prices and the\n",
        "# *actual* house prices, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"Avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df_house_att[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df_house_att[\"price\"].std(), grouping=True)))\n",
        "print(\"Mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "metadata": {
        "id": "4CKfxPXdCHIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some Thoughts on What We Did\n",
        "\n",
        "#### Training with substreams only\n",
        "For comparison:\n",
        "* Fusion: MAE 18% - 24%\n",
        "* DNN alone: MAE 22.71%\n",
        "* CNN alone: MAE 56.91%\n",
        "\n",
        "#### Data Set Size\n",
        "* Number of observations\n",
        "* Number of variables\n",
        "\n",
        "#### Information Flow\n",
        "* What is the dominant data stream?\n",
        "* Regress your results from one stream on the results of the other to see whether there is information left that can be modeled\n",
        "\n",
        "#### Optimizers\n",
        "* What is your favourite optimizer for CNN?\n",
        "* What is your favourite optimizer for text?\n",
        "* What is your favourite optimizer for micro data?\n",
        "\n",
        "#### How would you fine tune your model?\n",
        "* Transfer learning\n",
        "* Pre training\n"
      ],
      "metadata": {
        "id": "qP5Eeh0Fr2XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources:\n",
        "* https://github.com/emanhamed/Houses-dataset\n",
        "* https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
      ],
      "metadata": {
        "id": "_uuf3jhnTzrk"
      }
    }
  ]
}