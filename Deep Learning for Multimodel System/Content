Humans experience the world often with more than just one modality: for example, when we watch a speaker we see them, we listen to their words, and we also get information from their tone. As social scientists who seek to explain the social world around us, we would often like to make use of all this rich information and not "just" focus on one modality alone for our studies. Over the past decade, computer science has made important advances in multimodal machine learning. There is now a powerful toolkit that helps analyze multiple modalities at once.

This course offers an introduction into the basics of multimodal machine learning and discusses multimodal representations, their alignment, their fusion and how to make neural networks co-learn from multiple modalities. During the module you will understand the relevant core concepts of multimodal learning. Using key neural network architectures, you will be able to train joined representations and apply them to supervised classification tasks.
